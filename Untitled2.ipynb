{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/WjFLEuICUEqA3GB6z6Tq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinonyedoris/Applied-Learning-in-Natural-Language-Processing-NLP-/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYXUSOK7zSf8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ybQwI26ze2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ca3a67"
      },
      "source": [
        "# Task\n",
        "Explain the provided NLP assignments and generate Python code to complete the tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45df19bd"
      },
      "source": [
        "## Define nlp, applications, and challenges\n",
        "\n",
        "### Subtask:\n",
        "Provide a clear definition of NLP, list and explain real-world applications, and discuss challenges in a markdown cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96aeceb7"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a markdown cell to define NLP, list real-world applications, and discuss challenges as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "a05868d9",
        "outputId": "2019adfb-2e9f-48b1-b886-00866417630f"
      },
      "source": [
        "# %% [markdown]\n",
        "\"\"\"\n",
        "## Natural Language Processing (NLP)\n",
        "\n",
        "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. It involves developing algorithms and models that can process and analyze text and speech data, allowing machines to interact with humans in a more natural and intuitive way. Essentially, NLP bridges the gap between human communication and computer understanding.\n",
        "\n",
        "### Real-World Applications of NLP\n",
        "\n",
        "1.  **Sentiment Analysis:** This involves determining the emotional tone or sentiment expressed in a piece of text, such as positive, negative, or neutral. It's widely used in social media monitoring, customer feedback analysis, and market research to gauge public opinion.\n",
        "\n",
        "2.  **Machine Translation:** NLP powers tools like Google Translate, which automatically translate text or speech from one language to another. This is crucial for global communication, business, and accessing information across language barriers.\n",
        "\n",
        "3.  **Chatbots and Virtual Assistants:** These applications use NLP to understand user queries and provide relevant responses, simulating human conversation. They are commonly used in customer service, technical support, and as personal assistants (like Siri or Alexa).\n",
        "\n",
        "### Challenges in NLP\n",
        "\n",
        "1.  **Ambiguity:** Human language is often ambiguous, meaning a word or phrase can have multiple interpretations depending on the context. Resolving this ambiguity is a significant challenge for NLP systems. For example, the word \"bank\" can refer to a financial institution or the side of a river.\n",
        "\n",
        "2.  **Sarcasm and Irony:** Identifying and understanding sarcasm and irony in text is difficult for NLP models because they often involve saying the opposite of what is meant, relying heavily on tone and context that are not always apparent in written form.\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Natural Language Processing (NLP)\\n\\nNatural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. It involves developing algorithms and models that can process and analyze text and speech data, allowing machines to interact with humans in a more natural and intuitive way. Essentially, NLP bridges the gap between human communication and computer understanding.\\n\\n### Real-World Applications of NLP\\n\\n1.  **Sentiment Analysis:** This involves determining the emotional tone or sentiment expressed in a piece of text, such as positive, negative, or neutral. It\\'s widely used in social media monitoring, customer feedback analysis, and market research to gauge public opinion.\\n\\n2.  **Machine Translation:** NLP powers tools like Google Translate, which automatically translate text or speech from one language to another. This is crucial for global communication, business, and accessing information across language barriers.\\n\\n3.  **Chatbots and Virtual Assistants:** These applications use NLP to understand user queries and provide relevant responses, simulating human conversation. They are commonly used in customer service, technical support, and as personal assistants (like Siri or Alexa).\\n\\n### Challenges in NLP\\n\\n1.  **Ambiguity:** Human language is often ambiguous, meaning a word or phrase can have multiple interpretations depending on the context. Resolving this ambiguity is a significant challenge for NLP systems. For example, the word \"bank\" can refer to a financial institution or the side of a river.\\n\\n2.  **Sarcasm and Irony:** Identifying and understanding sarcasm and irony in text is difficult for NLP models because they often involve saying the opposite of what is meant, relying heavily on tone and context that are not always apparent in written form.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e574693"
      },
      "source": [
        "## Extract email addresses using regex\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to extract email addresses from a given text using regular expressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1865ad6a"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the `re` module, define the text and the regex pattern, and use `re.findall` to extract email addresses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ca9ff1",
        "outputId": "248248cc-694e-4390-a9c6-ffb9891a582a"
      },
      "source": [
        "import re\n",
        "\n",
        "text = \"Contact us at support@example.com or sales@anothersite.org for more information.\"\n",
        "pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "email_addresses = re.findall(pattern, text)\n",
        "print(email_addresses)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'sales@anothersite.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b715d51"
      },
      "source": [
        "## Extract words ending with \"ing\" using regex\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to extract words ending with \"ing\" from a given text using regular expressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dad811"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate Python code to extract words ending with \"ing\" from a given text using regular expressions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a3bc616",
        "outputId": "585252e4-37d2-45a2-8a90-7a7061e39498"
      },
      "source": [
        "import re\n",
        "\n",
        "text = \"Walking is a great way of exercising and improving your health by keeping fit and strong.\"\n",
        "pattern = r'\\b\\w*ing\\b'\n",
        "words_ending_with_ing = re.findall(pattern, text)\n",
        "print(words_ending_with_ing)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Walking', 'exercising', 'improving', 'keeping']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3022c9b"
      },
      "source": [
        "## Clean text using python\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to remove punctuation, convert to lowercase, and split the provided text into words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fef41974"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate Python code to define the text, import the string module, remove punctuation, convert to lowercase, split into words, and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a00dc5e",
        "outputId": "326ce053-1ede-47f6-aa47-1f31754d6136"
      },
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, world! This is a test sentence. Let's clean it.\"\n",
        "text_no_punctuation = \"\".join([char for char in text if char not in string.punctuation])\n",
        "text_lowercase = text_no_punctuation.lower()\n",
        "words = text_lowercase.split()\n",
        "print(words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'this', 'is', 'a', 'test', 'sentence', 'lets', 'clean', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee4b7487"
      },
      "source": [
        "## Text cleaning task\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to apply text cleaning techniques to the provided text, including removing punctuation, converting to lowercase, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d04187"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the text string, remove punctuation, convert to lowercase, and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2becc677",
        "outputId": "7408d9b5-10fc-4040-cd39-852bdcec4581"
      },
      "source": [
        "import string\n",
        "\n",
        "text = \"This is a sample text with some Punctuation! And UPPERCASE letters.\"\n",
        "text_no_punctuation = \"\".join([char for char in text if char not in string.punctuation])\n",
        "cleaned_text = text_no_punctuation.lower()\n",
        "print(cleaned_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a sample text with some punctuation and uppercase letters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aa41cf9"
      },
      "source": [
        "## Tokenization task\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to perform word-level and sentence-level tokenization using NLTK.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7cd1af3"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary NLTK modules, define a sample text, and perform word and sentence tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccdcecd3",
        "outputId": "d81a3c4c-88cf-4dbf-a32e-16b5ebd45ac3"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample sentence for tokenization. This is another sentence.\"\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word tokens:\", word_tokens)\n",
        "\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence tokens:\", sent_tokens)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.', 'This', 'is', 'another', 'sentence', '.']\n",
            "Sentence tokens: ['This is a sample sentence for tokenization.', 'This is another sentence.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e2d2e28"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the 'punkt' resource was not fully downloaded or accessible. The error message suggests downloading 'punkt_tab'. I will try downloading 'punkt' again, as 'punkt' should contain the necessary 'punkt_tab' data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tJjejhl04VO",
        "outputId": "a1777b5e-fcc6-4bef-e11f-6c27cd2ee689"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"This is a sample sentence for tokenization. This is another sentence.\"\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word tokens:\", word_tokens)\n",
        "\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence tokens:\", sent_tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.', 'This', 'is', 'another', 'sentence', '.']\n",
            "Sentence tokens: ['This is a sample sentence for tokenization.', 'This is another sentence.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e2dee2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to download 'punkt' failed to resolve the 'punkt_tab' issue. Following the explicit instruction in the error message, I will attempt to download 'punkt_tab' directly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oerK8-BZ06WP",
        "outputId": "2dd9925c-dfcd-4f6d-b912-66424330e910"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"This is a sample sentence for tokenization. This is another sentence.\"\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word tokens:\", word_tokens)\n",
        "\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence tokens:\", sent_tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.', 'This', 'is', 'another', 'sentence', '.']\n",
            "Sentence tokens: ['This is a sample sentence for tokenization.', 'This is another sentence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a9d678a"
      },
      "source": [
        "## Stemming and lemmatization task\n",
        "\n",
        "### Subtask:\n",
        "Stemming and lemmatization task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5043ac74"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries, define the word list, perform stemming using PorterStemmer, and print the stemmed words. Then, load the spaCy English model, perform lemmatization, and print the lemmatized words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0befa02",
        "outputId": "a3178992-bf19-48f7-dd07-e389a8d9cdc5"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "\n",
        "# Define a list of words\n",
        "word_list = [\"running\", \"ran\", \"runner\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in word_list]\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "lemmatized_words = [token.lemma_ for token in nlp(\" \".join(word_list))]\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['run', 'ran', 'runner', 'run', 'easili', 'fairli']\n",
            "Lemmatized words: ['run', 'run', 'runner', 'run', 'easily', 'fairly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fc2b848"
      },
      "source": [
        "## One-hot encoding\n",
        "\n",
        "### Subtask:\n",
        "Define a vocabulary and generate Python code to create one-hot encoded vectors for the vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9381ffd7"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the vocabulary, create a word-to-index mapping, and generate one-hot encoded vectors for each word in the vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a24aabb",
        "outputId": "04a2b54a-4c74-465d-e04f-aab869dc7066"
      },
      "source": [
        "vocabulary = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "vocab_size = len(vocabulary)\n",
        "one_hot_vectors = {}\n",
        "for word in vocabulary:\n",
        "  one_hot_vector = [0] * vocab_size\n",
        "  one_hot_vector[word_to_index[word]] = 1\n",
        "  one_hot_vectors[word] = one_hot_vector\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Word to index mapping:\", word_to_index)\n",
        "print(\"One-hot encoded vectors:\", one_hot_vectors)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['hello', 'world', 'this', 'is', 'a', 'test', 'sentence']\n",
            "Word to index mapping: {'hello': 0, 'world': 1, 'this': 2, 'is': 3, 'a': 4, 'test': 5, 'sentence': 6}\n",
            "One-hot encoded vectors: {'hello': [1, 0, 0, 0, 0, 0, 0], 'world': [0, 1, 0, 0, 0, 0, 0], 'this': [0, 0, 1, 0, 0, 0, 0], 'is': [0, 0, 0, 1, 0, 0, 0], 'a': [0, 0, 0, 0, 1, 0, 0], 'test': [0, 0, 0, 0, 0, 1, 0], 'sentence': [0, 0, 0, 0, 0, 0, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91b3166f"
      },
      "source": [
        "## Bag of words\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to create a Bag of Words representation for the provided sentences using CountVectorizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a254db2"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate Python code to create a Bag of Words representation for the provided sentences using CountVectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72318956",
        "outputId": "fa39de28-d3bf-4762-9903-ce9d4d3e81cd"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"This is the second sentence.\",\n",
        "    \"And this is the third one.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_words_matrix = vectorizer.fit_transform(sentences)\n",
        "bag_of_words_array = bag_of_words_matrix.toarray()\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words representation:\")\n",
        "print(bag_of_words_array)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'first' 'is' 'one' 'second' 'sentence' 'the' 'third' 'this']\n",
            "Bag of Words representation:\n",
            "[[0 1 1 0 0 1 1 0 1]\n",
            " [0 0 1 0 1 1 1 0 1]\n",
            " [1 0 1 1 0 0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ea86726"
      },
      "source": [
        "## Compute tf-idf\n",
        "\n",
        "### Subtask:\n",
        "Generate Python code to compute the TF-IDF for the provided sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e446b7c"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate Python code to compute the TF-IDF for the provided sentences using TfidfVectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aa1e6be",
        "outputId": "366627fb-c381-4750-951f-75567979d1f6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"This is the second sentence.\",\n",
        "    \"And this is the third one.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF representation:\")\n",
        "print(tfidf_array)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'first' 'is' 'one' 'second' 'sentence' 'the' 'third' 'this']\n",
            "TF-IDF representation:\n",
            "[[0.         0.61722732 0.3645444  0.         0.         0.46941728\n",
            "  0.3645444  0.         0.3645444 ]\n",
            " [0.         0.         0.3645444  0.         0.61722732 0.46941728\n",
            "  0.3645444  0.         0.3645444 ]\n",
            " [0.49711994 0.         0.29360705 0.49711994 0.         0.\n",
            "  0.29360705 0.49711994 0.29360705]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cadeece"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Natural Language Processing (NLP) is defined as a field of AI enabling computers to understand and process human language, with applications including Sentiment Analysis, Machine Translation, and Chatbots.\n",
        "*   Challenges in NLP include handling ambiguity and understanding sarcasm and irony.\n",
        "*   Regular expressions were successfully used to extract email addresses and words ending with \"ing\" from text.\n",
        "*   Text cleaning techniques, such as removing punctuation and converting text to lowercase, were implemented using Python.\n",
        "*   Word-level and sentence-level tokenization were performed using NLTK, requiring the download of the 'punkt\\_tab' resource.\n",
        "*   Stemming and lemmatization were demonstrated, showing the difference between reducing words to their root form (stemming) and their dictionary form (lemmatization).\n",
        "*   One-hot encoding was implemented to create binary vector representations of words based on a defined vocabulary.\n",
        "*   Bag of Words and TF-IDF representations of sentences were generated using `CountVectorizer` and `TfidfVectorizer` from scikit-learn, respectively, to represent text numerically based on word frequencies and importance.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The provided solutions cover fundamental NLP tasks, demonstrating basic text processing and feature extraction techniques.\n",
        "*   Building upon these foundational steps, the next steps could involve applying these processed text data and representations to machine learning models for tasks like text classification or clustering.\n"
      ]
    }
  ]
}